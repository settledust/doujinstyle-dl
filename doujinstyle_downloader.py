# ==============================================================================
#                  âœ¨ è¿è¡Œç¯å¢ƒè¦æ±‚ (Environment Requirements) âœ¨
# ==============================================================================
# 1. Python ç‰ˆæœ¬: 3.6+
# 2. ä¾èµ–åº“: pip install requests beautifulsoup4 lxml
# 3. åŠŸèƒ½ï¼šæ ¹æ®å…³é”®è¯å…¨è‡ªåŠ¨æŠ“å– doujinstyle ç½‘ç›˜é“¾æ¥ï¼Œæ”¯æŒ Mega, Mediafire, GD, PixelDrainã€‚
#    Function: Automatically crawl download links from doujinstyle based on keywords,
#    supporting Mega, Mediafire, Google Drive, and PixelDrain.
# ==============================================================================

import requests
from bs4 import BeautifulSoup
import re
import os
import time
import urllib.parse

# ==============================================================================
#                     ğŸš€ è‡ªåŠ¨åŒ–æå–è„šæœ¬é…ç½® (ç”¨æˆ·ä»…éœ€ä¿®æ”¹æ­¤å¤„) ğŸš€
#                     ğŸš€ Script Configuration (User just need to edit here) ğŸš€
# ==============================================================================

# ã€ç›®æ ‡é…ç½®ã€‘åªéœ€è¾“å…¥ doujinstyle çš„å±•ä¼šæ ‡ç­¾å…³é”®å­— (å¯¹åº” URL ä¸­çš„ result= å‚æ•°)ã€‚
# [Target Configuration] Just input the exhibition tag keyword (corresponds to result= parameter in URL).
#
# ç¤ºä¾‹ï¼šæ˜¥ä¾‹22ä¸ºrts22ï¼Œç§‹ä¾‹10ä¸ºarts10ï¼Œm3-2024æ˜¥ä¸ºm3-53ï¼ŒC104ä¸ºc104ï¼ŒC106ä¸œæ–¹Projectä¸ºc106%20touhou)
# Keyword example: ç¬¬äºŒåäºŒå›åšéº—ç¥ç¤¾ä¾‹å¤§ç¥­=rts22ï¼Œç¬¬åå›åšéº—ç¥ç¤¾ç§‹å­£ä¾‹å¤§ç¥­=arts10ï¼Œm3-2024æ˜¥=m3-53ï¼ŒC104=c104ï¼ŒC106æ±æ–¹Project=c106%20touhou
#
# è„šæœ¬ä¼šè‡ªåŠ¨å¤„ç† URL ç¼–ç ï¼ˆå¦‚ %20ï¼‰å¹¶ç”Ÿæˆå¯¹åº”çš„æ–‡ä»¶å¤¹å
# The script automatically handles URL encoding and generates safe filenames.

RESULT_KEYWORD = "c107%20touhou"

# ==============================================================================
#                      âš™ï¸ è‡ªåŠ¨åŒ–é€»è¾‘å¤„ç† (åº•å±‚æ ¸å¿ƒï¼Œæ— éœ€ä¿®æ”¹) âš™ï¸
#                      âš™ï¸ Logic Processing (Core, no edit needed) âš™ï¸
# ==============================================================================

# 1. ã€è·¯å¾„è‡ªé€‚åº”ã€‘è·å–å½“å‰è„šæœ¬è¿è¡Œçš„ç»å¯¹è·¯å¾„ã€‚
# [Path Adaptation] Get the absolute path of the current script.
current_dir = os.path.dirname(os.path.abspath(__file__))

# 2. ã€åŠ¨æ€å®‰å…¨å‘½åã€‘
# [Dynamic Safe Naming]
# urllib.parse.unquote: å°† '%20' è¿˜åŸä¸ºç©ºæ ¼ (Restore '%20' to spaces)
decoded_name = urllib.parse.unquote(RESULT_KEYWORD)

# re.sub: ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼å°† Windows/Linux ä¸å…è®¸çš„æ–‡ä»¶åç‰¹æ®Šå­—ç¬¦æ›¿æ¢ä¸ºä¸‹åˆ’çº¿
# Replace invalid filename characters for Win/Linux with underscores.
safe_name = re.sub(r'[\\/*?:"<>| ]', "_", decoded_name)
OUTPUT_FILENAME = f"links_{safe_name}.txt"

# txtæ–‡ä»¶å°†ç›´æ¥ç”Ÿæˆåœ¨è„šæœ¬åŒä¸€ç›®å½•ï¼Œè·¨å¹³å°ä¸”å½»åº•è§£å†³ Windows æ¡Œé¢è·¯å¾„è¿ç§»å¯¼è‡´çš„æŠ¥é”™
# TXT file generated in script directory to solve path errors across platforms.
OUTPUT_FILE_PATH = os.path.join(current_dir, OUTPUT_FILENAME)

# 3. ã€è¿æ¥ç®¡ç†ã€‘åˆ›å»º Session å¯¹è±¡ã€‚
# [Connection Management] Create Session object.
# Session çš„æ ¸å¿ƒä½œç”¨æ˜¯å®ç° TCP è¿æ¥å¤ç”¨ï¼ˆKeep-Aliveï¼‰ï¼Œåœ¨å¤§è§„æ¨¡è¯·æ±‚æ—¶èƒ½æå‡ 50% ä»¥ä¸Šçš„é€Ÿåº¦
# Sessions implement TCP Keep-Alive, boosting speed by 50%+ during large requests.
session = requests.Session()
session.headers.update(
    {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Referer": "https://doujinstyle.com/",
    }
)

# 4. ã€å¸¸é‡é…ç½®ã€‘
# [Constants Configuration]
BASE_URL = "https://doujinstyle.com/"
SEARCH_URL_TEMPLATE = "https://doujinstyle.com/?p=search&source=1&type=blanket&result={result_key}&page={page_num}"

# å­˜å‚¨å®¹å™¨ (Storage Container)
all_download_links = set()

# ==============================================================================
#                            â¬‡ï¸ æ ¸å¿ƒåŠŸèƒ½å‡½æ•° â¬‡ï¸
#                            â¬‡ï¸ Core Functions â¬‡ï¸
# ==============================================================================


def get_all_file_ids(result_key):
    """
    é€»è¾‘è¯´æ˜ï¼š
    Logic Description:
    1. éå†æœç´¢ç»“æœçš„æ¯ä¸€é¡µã€‚(Iterate through every search result page.)
    2. ä½¿ç”¨ BeautifulSoup é”å®š <mainbar> æ ‡ç­¾ï¼Œä»è€Œç‰©ç†éš”ç¦» <sidebar> ä¸­çš„â€œçƒ­é—¨ä¸“è¾‘â€ã€‚
       (Lock onto <mainbar> to isolate IDs from the "Hot Albums" in the sidebar.)
    3. å½“æ£€æµ‹åˆ°é¡µé¢ä¸å†äº§ç”Ÿæ–° ID æˆ–é¡µé¢å†…å®¹é‡å¤æ—¶ï¼Œè‡ªåŠ¨åœæ­¢ç¿»é¡µã€‚
       (Stop paging when no new IDs are found or content repeats.)
    """
    unique_ids = set()
    link_pattern = re.compile(r"\?p=page&type=1&id=(\d+)")
    current_page, previous_page_ids, max_duplicate_checks = 0, set(), 2

    print(f"--- ğŸ” æ­£åœ¨æ£€ç´¢ç»“æœ (Searching): {urllib.parse.unquote(result_key)} ---")

    while True:
        url = SEARCH_URL_TEMPLATE.format(result_key=result_key, page_num=current_page)
        current_page_ids = set()
        try:
            # å‘é€ GET è¯·æ±‚è·å–æœç´¢é¡µé¢å†…å®¹ (Send GET request)
            response = session.get(url, timeout=15)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, "lxml")

            mainbar = soup.find("mainbar")
            target_area = mainbar if mainbar else soup

            for a_tag in target_area.find_all("a", href=True):
                href = a_tag["href"]
                if "?p=page&type=1&id=" in href:
                    match = link_pattern.search(href)
                    if match:
                        current_page_ids.add(match.group(1))

            if not current_page_ids or (
                current_page > 0 and current_page_ids == previous_page_ids
            ):
                max_duplicate_checks -= 1
                if max_duplicate_checks == 0:
                    break

            if current_page_ids:
                unique_ids.update(current_page_ids)
                print(
                    f"  -> é¡µé¢ {current_page}: æˆåŠŸæå– {len(current_page_ids)} ä¸ªä¸“è¾‘ ID"
                )

            previous_page_ids = current_page_ids
            current_page += 1
            time.sleep(0.3)

        except Exception as e:
            print(f"  [Error] è®¿é—®ç¬¬ {current_page} é¡µæ—¶å‘ç”Ÿå¼‚å¸¸: {e}")
            break

    print(f"--- âœ… æ£€ç´¢å®Œæˆï¼Œå…±æ‰¾åˆ° {len(unique_ids)} ä¸ªç»“æœã€‚ ---")
    return sorted(list(unique_ids))


# ==============================================================================
#                            â¬‡ï¸ è„šæœ¬ä¸»æ‰§è¡Œæµç¨‹ â¬‡ï¸
#                            â¬‡ï¸ Main Execution â¬‡ï¸
# ==============================================================================

if __name__ == "__main__":
    start_time = time.time()

    # æ­¥éª¤ 1: è·å–æ‰€æœ‰å”¯ä¸€çš„ä¸“è¾‘ ID (Step 1: Get all unique Album IDs)
    FILE_IDS = get_all_file_ids(RESULT_KEYWORD)

    if not FILE_IDS:
        print("æœªå‘ç°ä»»ä½•åŒ¹é…çš„ IDï¼Œè¯·ç¡®è®¤ RESULT_KEYWORDã€‚")
    else:
        # æ­¥éª¤ 2: éå† IDï¼Œæ¨¡æ‹Ÿç‚¹å‡»â€œDownloadâ€æŒ‰é’®è·å–ç½‘ç›˜çœŸå®é“¾æ¥
        # Step 2: Traverse IDs, simulate "Download" click to get direct links.
        print(f"--- ğŸ“¡ æ­£åœ¨è§£æåŸå§‹ä¸‹è½½é“¾æ¥ (è§£ææ¨¡å¼: è¿æ¥å¤ç”¨) ---")

        for index, file_id in enumerate(FILE_IDS, 1):
            payload = {
                "type": "1",
                "id": file_id,
                "source": "0",
                "download_link": "Download",
            }
            try:
                response = session.post(
                    BASE_URL, data=payload, allow_redirects=False, timeout=20
                )
                if response.status_code in [301, 302, 303, 307]:
                    link = response.headers.get("Location")

                    # âœ¨ æ ¸å¿ƒæ”¹è¿›ï¼šå¢åŠ  PixelDrain æ”¯æŒ (Added PixelDrain support)
                    valid_hosts = [
                        "mega.nz",
                        "mediafire.com",
                        "drive.google.com",
                        "pixeldrain.com",
                    ]

                    if link and any(host in link for host in valid_hosts):
                        print(
                            f"  [{index}/{len(FILE_IDS)}] ID {file_id} -> é“¾æ¥æ•è·æˆåŠŸ"
                        )
                        all_download_links.add(link)
                else:
                    print(
                        f"  [{index}/{len(FILE_IDS)}] ID {file_id} -> æœªå‘ç°é‡å®šå‘é“¾æ¥"
                    )
            except Exception:
                continue

        # æ­¥éª¤ 3: æ±‡æ€»ç»“æœå¹¶è¾“å‡º (Step 3: Summarize and Output)
        if all_download_links:
            # âœ¨ ç»Ÿè®¡æ•°æ®å¢åŠ  PixelDrain é¡¹ (Added PixelDrain to stats)
            stats = {
                "Mega": sum(1 for k in all_download_links if "mega.nz" in k),
                "Mediafire": sum(1 for k in all_download_links if "mediafire.com" in k),
                "GoogleDrive": sum(
                    1 for k in all_download_links if "drive.google.com" in k
                ),
                "PixelDrain": sum(
                    1 for k in all_download_links if "pixeldrain.com" in k
                ),
            }

            with open(OUTPUT_FILE_PATH, "w", encoding="utf-8") as f:
                f.write("\n".join(sorted(list(all_download_links))))

            print(f"\n" + "=" * 55)
            print(f"ğŸ‰ ä»»åŠ¡å®Œæˆ (Success)! æ€»è€—æ—¶: {time.time() - start_time:.1f}s")
            print(
                f"ğŸ“Š åˆ†å¸ƒ (Stats): Mega({stats['Mega']}), Mediafire({stats['Mediafire']}), GD({stats['GoogleDrive']}), PD({stats['PixelDrain']})"
            )
            print(f"ğŸ”— æœ‰æ•ˆé“¾æ¥æ€»æ•° (Total Links): {len(all_download_links)}")
            print("=" * 55)
        else:
            print("\nâŒ ç»“æŸï¼Œæœªæå–åˆ°ä»»ä½•æœ‰æ•ˆçš„é“¾æ¥ã€‚")

# ==============================================================================
#                      ğŸ“Œ v1.0.1 æ–°å¢ï¼šç»å¯¹è·¯å¾„è¾“å‡ºæç¤º ğŸ“Œ
#                      ğŸ“Œ v1.0.1 New: Absolute Path Output Hint ğŸ“Œ
# ==============================================================================
try:
    if "OUTPUT_FILE_PATH" in locals() or "OUTPUT_FILE_PATH" in globals():
        if os.path.exists(OUTPUT_FILE_PATH):
            print(f"ğŸ“‚ ç»“æœæ–‡ä»¶ä¿å­˜è‡³ (Absolute Path):")
            print(f"ğŸ‘‰ {os.path.abspath(OUTPUT_FILE_PATH)}")
            print("=" * 55 + "\n")
except NameError:
    pass
