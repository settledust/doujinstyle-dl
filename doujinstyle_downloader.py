# ==============================================================================
#                  âœ¨ è¿è¡Œç¯å¢ƒè¦æ±‚ (Environment Requirements) âœ¨
# ==============================================================================
# 1. Python ç‰ˆæœ¬: 3.6+
# 2. ä¾èµ–åº“: pip install requests beautifulsoup4 lxml
# 3. åŠŸèƒ½ï¼šæ ¹æ®å…³é”®è¯å…¨è‡ªåŠ¨æŠ“å– doujinstyle ç½‘ç›˜é“¾æ¥ï¼Œæ”¯æŒ Session å¤ç”¨ã€è·¨å¹³å°è‡ªé€‚åº”è·¯å¾„ä¸æ™ºèƒ½å‘½åã€‚
#    Function: Automatically crawl download links from doujinstyle based on keywords, 
#    supporting session reuse, cross-platform path adaptation, and smart naming.
# ==============================================================================

import requests
from bs4 import BeautifulSoup
import re 
import os
import time
import urllib.parse

# ==============================================================================
#                     ğŸš€ è‡ªåŠ¨åŒ–æå–è„šæœ¬é…ç½® (ç”¨æˆ·ä»…éœ€ä¿®æ”¹æ­¤å¤„) ğŸš€
#                     ğŸš€ Script Configuration (User just need to edit here) ğŸš€
# ==============================================================================

# ã€ç›®æ ‡é…ç½®ã€‘åªéœ€è¾“å…¥ doujinstyle çš„å±•ä¼šæ ‡ç­¾å…³é”®å­— (å¯¹åº” URL ä¸­çš„ result= å‚æ•°)ã€‚
# [Target Configuration] Just input the exhibition tag keyword (corresponds to result= parameter in URL).
# 
# ç¤ºä¾‹ï¼šæ˜¥ä¾‹22ä¸ºrts22ï¼Œç§‹ä¾‹10ä¸ºarts10ï¼Œm3-2024æ˜¥ä¸ºm3-53ï¼ŒC104ä¸ºc104ï¼ŒC106ä¸œæ–¹Projectä¸ºc106%20touhou)
# Keyword example: ç¬¬äºŒåäºŒå›åšéº—ç¥ç¤¾ä¾‹å¤§ç¥­=rts22ï¼Œç¬¬åå›åšéº—ç¥ç¤¾ç§‹å­£ä¾‹å¤§ç¥­=arts10ï¼Œm3-2024æ˜¥=m3-53ï¼ŒC104=c104ï¼ŒC106æ±æ–¹Project=c106%20touhou
# 
# è„šæœ¬ä¼šè‡ªåŠ¨å¤„ç† URL ç¼–ç ï¼ˆå¦‚ %20ï¼‰å¹¶ç”Ÿæˆå¯¹åº”çš„æ–‡ä»¶å¤¹å
# The script automatically handles URL encoding and generates safe filenames.

RESULT_KEYWORD = 'c107%20touhou'

# ==============================================================================
#                      âš™ï¸ è‡ªåŠ¨åŒ–é€»è¾‘å¤„ç† (åº•å±‚æ ¸å¿ƒï¼Œæ— éœ€ä¿®æ”¹) âš™ï¸
#                      âš™ï¸ Logic Processing (Core, no edit needed) âš™ï¸
# ==============================================================================

# 1. ã€è·¯å¾„è‡ªé€‚åº”ã€‘è·å–å½“å‰è„šæœ¬è¿è¡Œçš„ç»å¯¹è·¯å¾„ã€‚
# [Path Adaptation] Get the absolute path of the current script.
current_dir = os.path.dirname(os.path.abspath(__file__))

# 2. ã€åŠ¨æ€å®‰å…¨å‘½åã€‘
# [Dynamic Safe Naming]
# urllib.parse.unquote: å°† '%20' è¿˜åŸä¸ºç©ºæ ¼ (Restore '%20' to spaces)
decoded_name = urllib.parse.unquote(RESULT_KEYWORD)

# re.sub: ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼å°† Windows/Linux ä¸å…è®¸çš„æ–‡ä»¶åç‰¹æ®Šå­—ç¬¦æ›¿æ¢ä¸ºä¸‹åˆ’çº¿
# Replace invalid filename characters for Win/Linux with underscores.
safe_name = re.sub(r'[\\/*?:"<>| ]', '_', decoded_name)
OUTPUT_FILENAME = f"links_{safe_name}.txt"

# txtæ–‡ä»¶å°†ç›´æ¥ç”Ÿæˆåœ¨è„šæœ¬åŒä¸€ç›®å½•ï¼Œè·¨å¹³å°ä¸”å½»åº•è§£å†³ Windows æ¡Œé¢è·¯å¾„è¿ç§»å¯¼è‡´çš„æŠ¥é”™
# TXT file generated in script directory to solve path errors across platforms.
OUTPUT_FILE_PATH = os.path.join(current_dir, OUTPUT_FILENAME)

# 3. ã€è¿æ¥ç®¡ç†ã€‘åˆ›å»º Session å¯¹è±¡ã€‚
# [Connection Management] Create Session object.
# Session çš„æ ¸å¿ƒä½œç”¨æ˜¯å®ç° TCP è¿æ¥å¤ç”¨ï¼ˆKeep-Aliveï¼‰ï¼Œåœ¨å¤§è§„æ¨¡è¯·æ±‚æ—¶èƒ½æå‡ 50% ä»¥ä¸Šçš„é€Ÿåº¦
# Sessions implement TCP Keep-Alive, boosting speed by 50%+ during large requests.
session = requests.Session()
session.headers.update({
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Referer': "https://doujinstyle.com/", 
})

# 4. ã€å¸¸é‡é…ç½®ã€‘
# [Constants Configuration]
BASE_URL = "https://doujinstyle.com/"
SEARCH_URL_TEMPLATE = "https://doujinstyle.com/?p=search&source=1&type=blanket&result={result_key}&page={page_num}"

# å­˜å‚¨å®¹å™¨ï¼šä½¿ç”¨ set ç¡®ä¿æ‰€æœ‰é“¾æ¥å¤©ç„¶å»é‡
# Storage: Use a set to ensure unique download links.
all_download_links = set()

# ==============================================================================
#                            â¬‡ï¸ æ ¸å¿ƒåŠŸèƒ½å‡½æ•° â¬‡ï¸
#                            â¬‡ï¸ Core Functions â¬‡ï¸
# ==============================================================================

def get_all_file_ids(result_key):
    """
    é€»è¾‘è¯´æ˜ï¼š
    Logic Description:
    1. éå†æœç´¢ç»“æœçš„æ¯ä¸€é¡µã€‚(Iterate through every search result page.)
    2. ä½¿ç”¨ BeautifulSoup é”å®š <mainbar> æ ‡ç­¾ï¼Œä»è€Œç‰©ç†éš”ç¦» <sidebar> ä¸­çš„â€œçƒ­é—¨ä¸“è¾‘â€ã€‚
       (Lock onto <mainbar> to isolate IDs from the "Hot Albums" in the sidebar.)
    3. å½“æ£€æµ‹åˆ°é¡µé¢ä¸å†äº§ç”Ÿæ–° ID æˆ–é¡µé¢å†…å®¹é‡å¤æ—¶ï¼Œè‡ªåŠ¨åœæ­¢ç¿»é¡µã€‚
       (Stop paging when no new IDs are found or content repeats.)
    """
    unique_ids = set()
    link_pattern = re.compile(r'\?p=page&type=1&id=(\d+)') # åŒ¹é…ä¸“è¾‘ ID çš„æ­£åˆ™ (Regex for Album IDs)
    current_page, previous_page_ids, max_duplicate_checks = 0, set(), 2

    print(f"--- ğŸ” æ­£åœ¨æ£€ç´¢ç»“æœ (Searching): {urllib.parse.unquote(result_key)} ---")

    while True:
        url = SEARCH_URL_TEMPLATE.format(result_key=result_key, page_num=current_page)
        current_page_ids = set()
        try:
            # å‘é€ GET è¯·æ±‚è·å–æœç´¢é¡µé¢å†…å®¹ (Send GET request)
            response = session.get(url, timeout=15)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'lxml')
            
            # âœ¨ æ ¸å¿ƒç­–ç•¥ï¼šç²¾å‡†é”å®šä¸»ä½“åŒºåŸŸã€‚
            # Core Strategy: Target the main content area specifically.
            mainbar = soup.find('mainbar')
            target_area = mainbar if mainbar else soup # å®¹é”™å¤„ç† (Fallback)
            
            for a_tag in target_area.find_all('a', href=True):
                href = a_tag['href']
                if '?p=page&type=1&id=' in href:
                    match = link_pattern.search(href)
                    if match:
                        current_page_ids.add(match.group(1))

            # ç¿»é¡µç»ˆæ­¢åˆ¤å®š (Paging termination logic)
            if not current_page_ids or (current_page > 0 and current_page_ids == previous_page_ids):
                max_duplicate_checks -= 1
                if max_duplicate_checks == 0: break
            
            if current_page_ids:
                unique_ids.update(current_page_ids)
                print(f"  -> é¡µé¢ {current_page}: æˆåŠŸæå– {len(current_page_ids)} ä¸ªä¸“è¾‘ ID")
            
            previous_page_ids = current_page_ids
            current_page += 1
            time.sleep(0.3) # ç¤¼è²Œé—´æ­‡ (Polite delay)

        except Exception as e:
            print(f"  [Error] è®¿é—®ç¬¬ {current_page} é¡µæ—¶å‘ç”Ÿå¼‚å¸¸: {e}"); break
            
    print(f"--- âœ… æ£€ç´¢å®Œæˆï¼Œå…±æ‰¾åˆ° {len(unique_ids)} ä¸ªç»“æœã€‚ ---")
    return sorted(list(unique_ids))

# ==============================================================================
#                            â¬‡ï¸ è„šæœ¬ä¸»æ‰§è¡Œæµç¨‹ â¬‡ï¸
#                            â¬‡ï¸ Main Execution â¬‡ï¸
# ==============================================================================

if __name__ == "__main__":
    start_time = time.time()
    
    # æ­¥éª¤ 1: è·å–æ‰€æœ‰å”¯ä¸€çš„ä¸“è¾‘ ID (Step 1: Get all unique Album IDs)
    FILE_IDS = get_all_file_ids(RESULT_KEYWORD)

    if not FILE_IDS:
        print("æœªå‘ç°ä»»ä½•åŒ¹é…çš„ IDï¼Œè¯·ç¡®è®¤ RESULT_KEYWORDã€‚ (No IDs found, check keyword.)")
    else:
        # æ­¥éª¤ 2: éå† IDï¼Œæ¨¡æ‹Ÿç‚¹å‡»â€œDownloadâ€æŒ‰é’®è·å–ç½‘ç›˜çœŸå®é“¾æ¥
        # Step 2: Traverse IDs, simulate "Download" click to get direct links.
        print(f"--- ğŸ“¡ æ­£åœ¨è§£æåŸå§‹ä¸‹è½½é“¾æ¥ (è§£ææ¨¡å¼: è¿æ¥å¤ç”¨) ---")
        
        for index, file_id in enumerate(FILE_IDS, 1):
            # payload æ¨¡æ‹Ÿäº†ç‚¹å‡»ä¸‹è½½æŒ‰é’®æ—¶å‘é€çš„ POST æ•°æ® (Simulate POST data)
            payload = {'type': '1', 'id': file_id, 'source': '0', 'download_link': 'Download'}
            try:
                # allow_redirects=False éå¸¸é‡è¦ï¼ç½‘ç›˜é“¾æ¥åœ¨ Location å“åº”å¤´ä¸­ã€‚
                # Crucial: disable redirects to catch the link in the Location header.
                response = session.post(BASE_URL, data=payload, allow_redirects=False, timeout=20)
                
                # æ£€æŸ¥æ˜¯å¦å‘ç”Ÿé‡å®šå‘ (Check for redirects 301-307)
                if response.status_code in [301, 302, 303, 307]:
                    link = response.headers.get('Location')
                    # è¿‡æ»¤é€»è¾‘ï¼šåªä¿ç•™å¸¸è§çš„ç½‘ç›˜é“¾æ¥ (Filtering: keep common hosts)
                    if link and any(host in link for host in ['mega.nz', 'mediafire.com', 'drive.google.com']):
                        print(f"  [{index}/{len(FILE_IDS)}] ID {file_id} -> é“¾æ¥æ•è·æˆåŠŸ")
                        all_download_links.add(link)
                else:
                    print(f"  [{index}/{len(FILE_IDS)}] ID {file_id} -> æœªå‘ç°é‡å®šå‘é“¾æ¥")
            except Exception: 
                continue

        # æ­¥éª¤ 3: æ±‡æ€»ç»“æœå¹¶è¾“å‡º (Step 3: Summarize and Output)
        if all_download_links:
            # ç®€å•çš„æ•°æ®åˆ†å¸ƒç»Ÿè®¡ (Statistics distribution)
            stats = {
                'Mega': sum(1 for k in all_download_links if 'mega.nz' in k),
                'Mediafire': sum(1 for k in all_download_links if 'mediafire.com' in k),
                'GoogleDrive': sum(1 for k in all_download_links if 'drive.google.com' in k)
            }
            
            # å°† set è½¬ä¸ºæ’åºåˆ—è¡¨å¹¶ä¿å­˜ (Sort and save results)
            with open(OUTPUT_FILE_PATH, 'w', encoding='utf-8') as f:
                f.write('\n'.join(sorted(list(all_download_links))))
            
            # æ‰“å°ä»»åŠ¡æŠ¥å‘Š (Print Task Report)
            print(f"\n" + "="*55)
            print(f"ğŸ‰ ä»»åŠ¡å®Œæˆ (Success)! æ€»è€—æ—¶: {time.time() - start_time:.1f}s")
            print(f"ğŸ“ æ–‡ä»¶å (File): {OUTPUT_FILENAME}")
            print(f"ğŸ“Š åˆ†å¸ƒ (Stats): Mega({stats['Mega']}), Mediafire({stats['Mediafire']}), GD({stats['GoogleDrive']})")
            print(f"ğŸ”— æœ‰æ•ˆé“¾æ¥æ€»æ•° (Total Links): {len(all_download_links)}")
            print("="*55)
        else:
            print("\nâŒ ç»“æŸï¼Œæœªæå–åˆ°ä»»ä½•æœ‰æ•ˆçš„é“¾æ¥ã€‚ (End, no valid links found.)")